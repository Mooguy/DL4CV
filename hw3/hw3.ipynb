{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "u3ku6Rrgxz-b"
   },
   "outputs": [],
   "source": [
    "#@title ## Mount Your Google Drive\n",
    "\n",
    "#@markdown The next two cells are **magic** cells.\n",
    "#@markdown They look like text cells, but they run code behind the scenes.\n",
    "#@markdown You can run them by either clicking on the ‚ñ∂Ô∏è button (to the left of the cell), or by clicking on the cell and typing `Ctrl+Enter` (or `Shift+Enter`).\n",
    "\n",
    "#@markdown Please run this cell and follow the steps printed after running it. Specifically, it will print a URL you should enter, follow the instructions there and paste the code in the textbox below (and type `Enter`).\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5iG-fCsoqvhu"
   },
   "outputs": [],
   "source": [
    "#@title ## Map Your Directory\n",
    "import os\n",
    "\n",
    "def check_assignment(assignment_dir, files_list):\n",
    "  files_in_dir = set(os.listdir(assignment_dir))\n",
    "  for fname in files_list:\n",
    "    if fname not in files_in_dir:\n",
    "      raise FileNotFoundError(f'could not find file: {fname} in assignment_dir')\n",
    "\n",
    "assignment_dest = \"/content/hw3\"\n",
    "assignment_dir = \"/content/gdrive/MyDrive/DL4CV/hw3\"  #@param{type:\"string\"}\n",
    "assignment_files = ['hw3.ipynb', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n",
    "                    'hw2_functional.py', 'hw2_nn.py', 'hw2_optim.py',\n",
    "                    'models.py', 'train.py', 'utils.py',\n",
    "                    'test_functional.py', 'test_nn.py', 'test_optim.py']\n",
    "\n",
    "# check Google Drive is mounted\n",
    "if not os.path.isdir(\"/content/gdrive\"):\n",
    "  raise FileNotFoundError(\"Your Google Drive isn't mounted. Please run the above cell.\")\n",
    "\n",
    "# check all files there\n",
    "check_assignment(assignment_dir, assignment_files)\n",
    "\n",
    "# create symbolic link\n",
    "!rm -f {assignment_dest}\n",
    "!ln -s \"{assignment_dir}\" \"{assignment_dest}\"\n",
    "print(f'Succesfully mapped (ln -s) \"{assignment_dest}\" -> \"{assignment_dir}\"')\n",
    "\n",
    "# cd to linked dir\n",
    "%cd -q {assignment_dest}\n",
    "print(f'Succesfully changed directory (cd) to \"{assignment_dest}\"')\n",
    "#@markdown Set the path `assignment_dir` to the assignment directory in your Google Drive and run this cell.\n",
    "\n",
    "#@markdown If you are not sure what is the path, you can use the **Files (üìÅ)** menu (on the left side) to check the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIfpUXsh9qmW"
   },
   "source": [
    "## Imports and `autoreload`-Magic\n",
    "Please run the cell below (only once) to load and set the `autoreload` magic, which automatically reloads the import calls to the python files with your solutions. That means that you can edit the files (in the right-side window), save them (`Ctrl+S`) and just re-run the relevant cells -- the new code will kick in automatically.\n",
    "\n",
    "**Note:** You **MUST NOT** install any package. If you can't load something, you probably didn't follow the instructions (either didn't uploaded all the files, didn't mounted your Google driver or didn't mapped your directory).\n",
    "\n",
    "**Note:** The exercise works as is. If you add or modify imports to things, it may break thing in the notebook. You may do so **AT YOUR OWN RISK**. We will not assist with issues in notebook with modified imports.\n",
    "\n",
    "**Note:** Make sure you run **all the cells** up to the point. Some cells depends on previous cells (mainly imports). Furthermore, make sure to run the cell below (with the autoreload magic) before any cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x_Xg1LYZlnQ9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/labs/antebilab/guyilan/Courses/DL4CV/hw3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rwfbzEQnoXr"
   },
   "source": [
    "# (A) Written Assignment\n",
    "\n",
    "In addition to the coding assignment, there is also a theoretical written assignment that can be found in `hw3.pdf`. \n",
    "Please solve this assignment and upload your solution to the google drive folder as `hw3-sol.pdf`. It will be packed together with your coding solution in the **Submit Your Solution** section.\n",
    "\n",
    "Your solution to the written part should be typed, not hand-written. We recommend using LyX or LaTex, but you can also use Word or similar text editor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc5m_BNxNf31"
   },
   "source": [
    "# (B) Implement CNN from scratch\n",
    "In This part you will implement a deep **convolutional** neural network from scratch, including the necessary building blocks (similarly to HW2). You will implement it in the following order:\n",
    "1. **Differentiable Functions:** a set of differentiable functions that are used as atomic building blocks.\n",
    "2. **Learnable Layers:** Conolutional layer and MaxPool layer.\n",
    "3. **Optimizer:** SGD *with momentum* (building on your previous vanilla SGD implementation of HW2).\n",
    "\n",
    "Note that many functions you have implemented in HW2 will be useful in this assignment. They are already imported (since you uploaded `hw2_functional.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u32VOx2k3Pb8"
   },
   "source": [
    "## (B.1) Differentiable Functions\n",
    "\n",
    "In this section you will implement a set of differentiable functions from scratch. For each function, you will implement the forward and backward methods. After the description of the method, there is a testing cell which we will test the correctness of your code.\n",
    "\n",
    "The skeletons of the differential functions to implement are in the `functional.py` file. Open this file by clicking on this link: `/content/hw3/functional.py`. Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw3` (or `content/hw3`) and double-click on `functional.py` to open it. The tests can be found in `test_functional.py` (link: `/content/hw3/test_functional.py`).\n",
    "\n",
    "In each step you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, for your code to take effect, make sure you save `.py` files using `Ctrl+S`.\n",
    "\n",
    "### Reminder - `ctx`\n",
    "As in HW2, in the \"from scratch\" implementation, you should use a `ctx` (context) variable. This variable is needed for the back propagation algorithm.\n",
    "\n",
    "Specifically, `ctx` is just a list (or stack) of \"backward calls\", where each \"backward call\" is a pair (list/tuple) of two objects:\n",
    "\n",
    "1. **`backward_fn`:** The backward function. A reference to the backward function to be called in the backward pass.\n",
    "2. **`args`:** A list (or tuple) of arguments to be passed to `backward_fn`. This list usually consists of the inputs and the outputs of the forward function. Sometimes additional arguments are passed as well. It's important to pass the actual inputs and outputs (same pointer), otherwise it would break the chain of gradients propagation.\n",
    "\n",
    "The \"backward calls\" in `ctx` should be ordered in according to the time of addition. That is, a backward call that was added later should have an higher index in the list `ctx`. If `ctx` is `None`, it means that gradients (i.e. backward calls) should not be tracked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRhfmjXODEo"
   },
   "source": [
    "### (B.1.1) Implement the `conv2d` Function\n",
    "\n",
    "Here you will implement a differentiable `conv2d` function. This includes the forward `conv2d` function and the backward `conv2d_backward` function. \n",
    "\n",
    "**Note:** Your solution (both `conv2d` and `conv2d_backward`) should be vectorized. Specifically, consider the `fold`, `unfold`, `einsum` functions of pytorch for vectorized solutions.\n",
    "\n",
    "#### `conv2d`\n",
    "The `conv2d` function receives five arguments (in addition to the autograd context `ctx`):\n",
    "\n",
    "  * `x`: The batched input. Has shape `(batch_size, in_channels, in_height, in_width)`.\n",
    "  * `w`: The convolution kernel. Has shape `(out_channels, in_channels, kernel_height, kernel_width)`.\n",
    "  * `b`: The bias term. Has shape `(out_channels,)`.\n",
    "  * `padding`: The padding in each dimension, has shape (`width_padding`, `height_padding`), or an int representing same padding in both dimensions.\n",
    "  * `stride`: The stride in each dimension, has shape (`width_stride`, `height_stride`), or an int representing same stride in both dimensions.\n",
    "  * `dilation` (**Bonus**): The dilation in each dimension, has shape (`width_dilation`, `height_dilation`), or an int representing same dilation in both dimensions.\n",
    "  * `groups` (**Bonus**): Division of channels into groups. Read more in [Conv2d documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) of Pytorch.\n",
    "\n",
    "It computes the (batched version of the) function: $$ \\mathbf{y} = W*\\mathbf{x} + \\mathbf{b} $$\n",
    "Note that the output `y` has shape which depends on the input parameters.\n",
    "\n",
    "#### `conv2d_backward`\n",
    "The `conv2d_backward` function receives the following arguments:\n",
    "\n",
    "  * `y`: The batched output. Has shape `(batch_size, out_channels, out_height, out_width)`.\n",
    "  * `x`: The batched input. Has shape `(batch_size, in_channels, in_height, in_width)`.\n",
    "  * `w`: The weight matrix. Has shape `(out_channels, in_channels, kernel_height, kernel_width)`.\n",
    "  * `b`: The bias term. Has shape `(out_channels,)`.\n",
    "  * `padding`: The padding in each dimension, has shape (`width_padding`, `height_padding`), or an int representing same padding in both dimensions.\n",
    "  * `stride`: The stride in each dimension, has shape (`width_stride`, `height_stride`), or an int representing same stride in both dimensions.\n",
    "  * `dilation` (**Bonus**): The dilation in each dimension, has shape (`width_dilation`, `height_dilation`), or an int representing same dilation in both dimensions.\n",
    "  * `groups` (**Bonus**): Division of channels into groups. Read more in [Conv2d documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) of Pytorch.\n",
    "\n",
    "It computes the gradients of `x`, `w` and `b` w.r.t the loss, given the gradient of `y` (in `y.grad`), and accumulates these gradients in `x.grad`, `w.grad` and `b.grad`, respectively. Note that `stride` and `padding` also have an effect on the backward calculation.\n",
    "\n",
    "**Note:** If you choose not to do the bonus `groups` and `dilation` arguments, you may assume that their value is always 1 (their default value in the function). Note that some tests may fail in this case (with groups/dilation in their names).\n",
    "\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OkrZhq0U31I0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\n",
      "----------------------------------------------------------------------\n",
      "Ran 17 tests in 0.851s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestConv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UAqdVJl9uxrI"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging conv2d\n",
    "from functional import conv2d, conv2d_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3vGFltjkDSm"
   },
   "source": [
    "### (B.1.2) Implement the `max_pool2d` function\n",
    "\n",
    "Here you will implement a differentiable MaxPool activation. This includes the forward `max_pool2d` function and the backward `max_pool2d_backward` function.\n",
    "\n",
    "**Note:** Your solution should be vectorized. Please consider `fold`, `unfold`, `gather` and `scatter_add_` functions for a vectorized solution.\n",
    "\n",
    "#### `max_pool2d`\n",
    "\n",
    "\n",
    "\n",
    "The `max_pool2d` function receives the following arguments:\n",
    "\n",
    "  * `x`: The input. Has shape `(batch_size, in_channels, in_height, in_width)`.\n",
    "  * `kernel_size`: The window size which should be maxpooled, with shape `(width,height)`.\n",
    "  * `padding`: The padding in each dimension, has shape (`width_padding`, `height_padding`), or an int representing same padding in both dimensions.\n",
    "  * `stride`: The stride in each dimension, has shape (`width_stride`, `height_stride`), or an int representing same stride in both dimensions.\n",
    "  * `dilation` (**Bonus**): The dilation in each dimension, has shape (`width_dilation`, `height_dilation`), or an int representing same dilation in both dimensions.\n",
    "\n",
    "The output `y` is the max value in each window of size `kernel_size` (The windows locations are affected by the stride and padding as well).\n",
    "Note that the output `y` has shape which depends on the input parameters.\n",
    "\n",
    "#### `max_pool2d_backward`\n",
    "The `max_pool2d_backward` function receives two arguments:\n",
    "\n",
    "  * `y`: The output. has shape `(batch_size, in_channels, out_height, out_width)`.\n",
    "  * `x`: The input. \n",
    "  * `index`: The indices of the maximum values in each window.\n",
    "  * `kernel_size`: The window size which should be maxpooled, with shape `(width,height)`.\n",
    "  * `padding`: The padding in each dimension, has shape (`width_padding`, `height_padding`), or an int representing same padding in both dimensions.\n",
    "  * `stride`: The stride in each dimension, has shape (`width_stride`, `height_stride`), or an int representing same stride in both dimensions.\n",
    "  * `dilation` (**Bonus**): The dilation in each dimension, has shape (`width_dilation`, `height_dilation`), or an int representing same dilation in both dimensions.\n",
    "\n",
    "\n",
    "It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`), and accumulates this gradient in `x.grad`.\n",
    "\n",
    "**Note:** If you choose not to do the bonus `dilation` argument, you may assume that its value is always 1 (its default value in the function). Note that some tests may fail in this case (with dilation in their names).\n",
    "\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lA-NX0Y-j8yF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "----------------------------------------------------------------------\n",
      "Ran 15 tests in 0.761s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestMaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m_79D4JGj888"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging max_pool2d\n",
    "from functional import max_pool2d, max_pool2d_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8Ky6e70tEHb"
   },
   "source": [
    "### (B.1.3) Implement the `view` Function\n",
    "\n",
    "Here you will implement a differentiable `view` function. This function takes an input and changes its shape. It will be required when training a CNN, since the input is 2D, but the output should be 1D vector. You may use the `view` function of pytorch in your implementation.\n",
    "\n",
    "#### `view`\n",
    "The `view` function receives:\n",
    "\n",
    "  * `x`: Input, with arbitrary shape.\n",
    "  * `size`: The output shape.\n",
    "\n",
    "#### `view_backward`\n",
    "The `view_backward` function receives the following arguments:\n",
    "\n",
    "  * `y`: The output with arbitrary shape. \n",
    "  * `x`: The input.\n",
    "\n",
    "\n",
    "It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`), and accumulates these gradients in `x.grad`.\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1Gt20GQIuI9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.559s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xy1UQPoujslF"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging view\n",
    "from functional import view, view_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDZglBCQMZpp"
   },
   "source": [
    "### (B.1.4) Implement the `add` Function\n",
    "\n",
    "Here you will implement a differentiable `add` function. This function takes two inputs with the same size and adds them. It will be required for designing a neural network with residual connections.\n",
    "\n",
    "#### `add`\n",
    "The `add` function receives:\n",
    "\n",
    "  * `a`: Input, with arbitrary shape.\n",
    "  * `b`: Input, should have the same shape as `a`.\n",
    "\n",
    "#### `add_backward`\n",
    "The `add_backward` function receives the following arguments:\n",
    "\n",
    "  * `y`: The output with arbitrary shape. \n",
    "  * `a`: The first input.\n",
    "  * `b`: The second input.\n",
    "\n",
    "It computes the gradients of `a`,`b` w.r.t the loss, given the gradient of `y` (in `y.grad`), and accumulates these gradients in `a.grad`, `b.grad`.\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_jL23eV3NCDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.540s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UgJrt3pANEu5"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging add\n",
    "from functional import add, add_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e_g4fDakFoQ"
   },
   "source": [
    "## (B.2) Autograd\n",
    "The `autograd.py` file you have implemented as part of HW2 should be on the folder, you will use it again in this assignment.\n",
    "\n",
    "Reminder:\n",
    "\n",
    "`autograd.py` contains a general `backward` method. This method stands at the core of back-propagation.\n",
    "\n",
    "This method receives two arguments:\n",
    "\n",
    "* `loss`: The loss tensor. This tensor must be a scalar (Has shape `()`). The loss the other tensors will be computed w.r.t this `loss`.\n",
    "* `ctx`: The autograd context. A list of backward calls. These backward calls should be evaluated to back-propagate the gradient from `loss` to the tensors used in the computation of `loss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F30SfhnmkWP"
   },
   "source": [
    "## (B.3) Layers\n",
    "\n",
    "So far in this exercise, you have implemented *stateless* differentiable functions. In this section, you will implement *stateful* layers, with parameters. \n",
    "\n",
    "In this section you will implement a learnable layer `Conv2d`. You will also implement a non-learnable layer - `MaxPool2d` (it has non-learnable parameters that we wish to store in a per-layer fashion). The implementation is similar to vanilla PyTorch.\n",
    "\n",
    "You may use any layer you have implemented in HW2 in this assignment. It is already imported (since you uploaded `hw2_nn.py`).\n",
    "\n",
    "The skeleton of the layers to implement is in the `nn.py` file (link: `/content/hw3/nn.py`). The tests can be found in `test_nn.py` (link: `/content/hw3/test_nn.py`).\n",
    "\n",
    "Reminder: Layers (and networks) inherits from the provided class `Module` (which is similar to PyTorch's `nn.Module`). This abstract class implements some utility methods. \n",
    "\n",
    "In the `nn.py` file, you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). \n",
    "\n",
    "In your layers, you should:\n",
    "\n",
    "1. **Create parameter tensors:** create tensors for the **learnable** parameters in the correct shape. The parameters should be attributes of the layer, i.e. set as `self.<param> = <tensor>`. This is done in `__init__`.\n",
    "2. **Register learnable parameters:** add their names to `self._parameters`. This will be used by the provided `Module.parameters()` (to list module's parameters) and `Module.to()` (to trasfer module's parameters to a device) methods. This is done in `__init__`.\n",
    "3. **Initialize learnable parameters:** initialization of the layer parameters has significant influence on the local minimum the network reaches during training. This is done in `init_parameters()`. You should call this method from `__init__`, so newly created linear layers are initialized.\n",
    "4. **Store non-learnable arguments:** Create instance attributes (i.e. `self.<something>`) to hold the non-learnable arguments. These will be used to call the stateless functions implemented in B.1 with the correct parameters.\n",
    "5. **Implement a forward method:** use the existing differentiable function from part A, and implement the `forward()` method.\n",
    "\n",
    "\n",
    "**Note:** Since this part doesn't use PyTorch's built-in autograd mechanism, please do not use tensors' `requires_grad` (this will result in errors/warnings).\n",
    "Furthermore, do not use `nn.Parameter` in _from scratch_ layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkxNHgUnAoa"
   },
   "source": [
    "### (B.3.1) Implement `Conv2d` Layer\n",
    "\n",
    "The learnable parameters of the `Conv2d` layer are the convolution kernels `weight`, and the bias term `bias`. Note that there are non-learnable arguments which should be stored in your layers (e.g. `stride`).\n",
    "\n",
    "**Note:** When the `Conv2d` doesn't require `bias`, do not store it in `self._parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ARMFodrXnBfW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.028s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_nn.TestConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wENev5CtcjnT"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging Conv2d\n",
    "from nn import Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ37CZONywRY"
   },
   "source": [
    "### (B.3.2) Implement `MaxPool2d` Layer\n",
    "\n",
    "The `MaxPool2d` layer doesn't have learnable parameters. Note that there are non-learnable arguments which should be stored in your layers (e.g. `stride`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zapu817jyxND"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_nn.TestMaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WFwE982HyxsL"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging MaxPool2d\n",
    "from nn import MaxPool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTKP7Ovas-_M"
   },
   "source": [
    "## (B.4) Optimizer\n",
    "\n",
    "In this section you will implement an advanced optimizer. In HW2, a vanilla SGD optimizer has been implemented. In this assignment, you will implement *SGD with momentum*. Reminder - The optimizer has three main functions:\n",
    "\n",
    "1. `__init__`: Receives the list of parameters (weights) to update their values and save them. May receive additional arguments, such as learning-rate and **momentum**.\n",
    "2. `step`: Updates the parameters values based on the value of their gradients. Doesn't receive any argument.\n",
    "3. `zero_grad`: Zeros the gradients of the tracked parameters. This is necessary since gradients are accumulated in each backward pass, and we don't want to mix between batches. Doesn't receive any argument.\n",
    "\n",
    "The skeleton of the optimizer is in the `optim.py` file (link: `/content/hw3/optim.py`). The tests can be found in `test_optim.py` (link: `/content/hw3/test_optim.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK5kaf5kmoxM"
   },
   "source": [
    "### (B.4.1) MomentumSGD Optimizer\n",
    "In this part, you'll implement an MomentumSGD optimizer. This optimizer has the following update rule:\n",
    "$$\n",
    "\\mathbf{v}_{0} \\leftarrow \\mathbf{0}\n",
    "\\\\\n",
    "\\mathbf{v}_{n+1} \\leftarrow \\mu \\cdot \\mathbf{v}_{n} + \\mathbf{g}_{n+1}\n",
    "\\\\\n",
    "\\mathbf{x}_{n+1} \\leftarrow \\mathbf{x}_{n} - \\eta \\cdot \\mathbf{v}_{n+1}\n",
    "$$\n",
    "Where $\\mathbf{x}_{n}$ is the parameter at step $n$, $\\mathbf{g}_{n}$ is its gradient at step $n$, $\\eta$ is the learning rate (also called `lr`), and $\\mu$ is the momentum parameter.\n",
    "\n",
    "You should implement the `__init__`, `step` and `zero_grad` methods of `SGD` optimizer in `optim.py`. You are encouraged to base you solution on your SGD solution of HW2.\n",
    "\n",
    "**Note:** Parameters (tensors) should be updated **in-place** (i.e. with the `-=` operator) in `step`.\n",
    "\n",
    "**Note:** A gradient (`param.grad`) which is set to `None` is also considered as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iR4bRY16aW1Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labs/antebilab/guyilan/Courses/DL4CV/hw3/test_optim.py:29: FutureWarning: `torch.testing.assert_allclose()` is deprecated since 1.12 and will be removed in a future release. Please use `torch.testing.assert_close()` instead. You can find detailed upgrade instructions in https://github.com/pytorch/pytorch/issues/61844.\n",
      "  torch.testing.assert_allclose(param, ref_param, msg=dbg)\n",
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 1.064s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_optim.TestMomentumSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dZlZwEqGj9_2"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging MomentumSGD\n",
    "from optim import MomentumSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XStdhdlPcSBd"
   },
   "source": [
    "# Setup Before Training\n",
    "\n",
    "In this part you will need to use GPU (this will have a significant impact on the training speed). To get a GPU in Google Colab, please go to the top menu and to: **Runtime ‚ûî Change runtime type**. Then, select **GPU** as **Hardware accelerator**.\n",
    "\n",
    "Please run the cell below to set your pytorch device (either GPU or CPU), to load the dataset and to create data loaders.\n",
    "\n",
    "In HW2, the dataset was the MNIST digits dataset (where you classified black and white digits). In this exercise, you will be required to classifiy RGB images into ten categories, using the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. These are RGB images (i.e. has 3 channels) of size $32 \\times 32$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cVRcw-dlgFLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils import load_cifar10\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pin_memory = device.type == 'cuda'\n",
    "\n",
    "# Load the training and test sets\n",
    "train_data = load_cifar10(mode='train')\n",
    "test_data = load_cifar10(mode='test')\n",
    "\n",
    "# Create dataloaders for training and test sets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiJaEoARnAOn"
   },
   "source": [
    "# (C) Define and Train Convolutional Neural Networks From Scratch\n",
    "\n",
    "\n",
    "In this part, you will define and train neural networks from scratch. You will use your differentiable functions from section (B).\n",
    "\n",
    "The skeletons for this assignment can be found in the `models.py` (link: `/content/hw3/models.py`) and `train.py` (link: `/content/hw3/train.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. \n",
    "\n",
    "### `train.py`\n",
    "Your solution for `train.py` should be very similar to your solution of HW2 (except for a minor change).\n",
    "\n",
    "**Note:** Vectorized implementation of the differntiable functions (in `functional.py` and `hw2_functional.py`) has a dramatic effect on the training speed. If your solution to hw2 (i.e. in `hw2_functional.py`) was not vectorized, considered re-implementing it vectorically. You may consult and share vectorized code **of the already submitted hw2** with your colleagues.\n",
    "\n",
    "### `models.py`\n",
    "Here you will implement the `ConvNet`.\n",
    "\n",
    "In /content/hw3/models.py, define the architecture (depth, width, etc.) in the `__init__` function. Remember to register every layer in the following way: `self._modules = ['conv1', 'conv2', ..., 'fc']`. \n",
    "\n",
    "The `forward` method will be used for defining the forward pass in your CNN.\n",
    "\n",
    "---\n",
    "\n",
    "Once finished, please run the cell below to import the relevant objects in order to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PzTMHw9yKxN6"
   },
   "outputs": [],
   "source": [
    "from functional import cross_entropy_loss as cross_entropy_scratch\n",
    "from models import ConvNet\n",
    "from optim import MomentumSGD\n",
    "from train import train_loop as train_loop_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW255-xmnA6O"
   },
   "source": [
    "## (C.1) Implement and Train a ConvNet\n",
    "\n",
    " \n",
    "\n",
    "Once the network is designed, use the following cell to train the network. This includes the following parts:\n",
    "\n",
    "1. Create the model.\n",
    "2. (Optional) Transfer the model to `device`.\n",
    "3. Create an optimizer, and set its params. (this should be done when the model is in its final device. It will not work otherwise).\n",
    "4. Set other hyper-parameters (loss function, number of epochs, etc.).\n",
    "5. Train the model.\n",
    "\n",
    "Your goal is to reach high accuracy (>65%). Achieving substantially higher accuracy (>80%) will be awarded with bonus. You are encouraged to fine tune the following:\n",
    "* Network architecture (number of layers, convolution parameters, number of channels, etc.)..\n",
    "* Weights initialization.\n",
    "* Optimizer parameters.\n",
    "* Number of epochs.\n",
    "\n",
    "Furthermore, you may also try other ways to optimize your performance, such as:\n",
    "* Adding dilation and groups to your conv implementation.\n",
    "* Using residual-connections in your network.\n",
    "* Changing the learning rate during training (called scheduling, can be done in the notebook, by calling training loop several times)\n",
    "* Implementing and adding `BatchNorm2d` ([read this before](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)). <br>**Note:** This is very technically difficult and time demanding. You would have to implement `functional.batchnorm2d`, `functional.batchnorm2d_backward` and `nn.BatchNorm2d`. The `mu` and `sigma` should be accumulated in **buffers** (`self._buffers`) of `nn.BatchNorm2d`, not **parameters**. You may implement additional auxilary differentiable functions for that task, and consult with us regardind this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQlsopQc3dF5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 200   Loss:   2.302   Accuracy: 0.117\n",
      " Test   Epoch: 001 / 200   Loss:     2.3   Accuracy: 0.166\n",
      "Train   Epoch: 002 / 200   Loss:   2.152   Accuracy: 0.211\n",
      " Test   Epoch: 002 / 200   Loss:   1.983   Accuracy: 0.259\n",
      "Train   Epoch: 003 / 200   Loss:   1.791   Accuracy: 0.340\n",
      " Test   Epoch: 003 / 200   Loss:   1.638   Accuracy: 0.400\n",
      "Train   Epoch: 004 / 200   Loss:   1.553   Accuracy: 0.435\n",
      " Test   Epoch: 004 / 200   Loss:   1.481   Accuracy: 0.464\n",
      "Train   Epoch: 005 / 200   Loss:   1.412   Accuracy: 0.494\n",
      " Test   Epoch: 005 / 200   Loss:    1.33   Accuracy: 0.520\n",
      "Train   Epoch: 006 / 200   Loss:   1.275   Accuracy: 0.546\n",
      " Test   Epoch: 006 / 200   Loss:   1.227   Accuracy: 0.566\n",
      "Train   Epoch: 007 / 200   Loss:   1.149   Accuracy: 0.594\n",
      " Test   Epoch: 007 / 200   Loss:   1.142   Accuracy: 0.601\n",
      "Train   Epoch: 008 / 200   Loss:   1.046   Accuracy: 0.633\n",
      " Test   Epoch: 008 / 200   Loss:   1.049   Accuracy: 0.634\n",
      "Train   Epoch: 009 / 200   Loss:  0.9551   Accuracy: 0.666\n",
      " Test   Epoch: 009 / 200   Loss:    1.02   Accuracy: 0.647\n",
      "Train   Epoch: 010 / 200   Loss:  0.8812   Accuracy: 0.692\n",
      " Test   Epoch: 010 / 200   Loss:  0.9216   Accuracy: 0.680\n",
      "Train   Epoch: 011 / 200   Loss:  0.8154   Accuracy: 0.716\n",
      " Test   Epoch: 011 / 200   Loss:  0.8551   Accuracy: 0.708\n",
      "Train   Epoch: 012 / 200   Loss:  0.7541   Accuracy: 0.739\n",
      " Test   Epoch: 012 / 200   Loss:  0.8583   Accuracy: 0.711\n",
      "Train   Epoch: 013 / 200   Loss:  0.7008   Accuracy: 0.757\n",
      " Test   Epoch: 013 / 200   Loss:  0.8605   Accuracy: 0.710\n",
      "Train   Epoch: 014 / 200   Loss:  0.6519   Accuracy: 0.774\n",
      " Test   Epoch: 014 / 200   Loss:  0.7674   Accuracy: 0.738\n",
      "Train   Epoch: 015 / 200   Loss:  0.5998   Accuracy: 0.792\n",
      " Test   Epoch: 015 / 200   Loss:   0.724   Accuracy: 0.753\n",
      "Train   Epoch: 016 / 200   Loss:  0.5565   Accuracy: 0.807\n",
      " Test   Epoch: 016 / 200   Loss:  0.7241   Accuracy: 0.754\n",
      "Train   Epoch: 017 / 200   Loss:  0.5163   Accuracy: 0.822\n",
      " Test   Epoch: 017 / 200   Loss:  0.8104   Accuracy: 0.736\n",
      "Train   Epoch: 018 / 200   Loss:  0.4799   Accuracy: 0.834\n",
      " Test   Epoch: 018 / 200   Loss:   0.727   Accuracy: 0.759\n",
      "Train   Epoch: 019 / 200   Loss:   0.439   Accuracy: 0.848\n",
      " Test   Epoch: 019 / 200   Loss:  0.7454   Accuracy: 0.755\n",
      "Train   Epoch: 020 / 200   Loss:  0.4015   Accuracy: 0.861\n",
      " Test   Epoch: 020 / 200   Loss:  0.7717   Accuracy: 0.758\n",
      "Train   Epoch: 021 / 200   Loss:  0.3664   Accuracy: 0.873\n",
      " Test   Epoch: 021 / 200   Loss:   0.765   Accuracy: 0.759\n",
      "Train   Epoch: 022 / 200   Loss:  0.3345   Accuracy: 0.882\n",
      " Test   Epoch: 022 / 200   Loss:  0.7996   Accuracy: 0.758\n",
      "Train   Epoch: 023 / 200   Loss:   0.301   Accuracy: 0.895\n",
      " Test   Epoch: 023 / 200   Loss:  0.7932   Accuracy: 0.765\n",
      "Train   Epoch: 024 / 200   Loss:   0.268   Accuracy: 0.905\n",
      " Test   Epoch: 024 / 200   Loss:  0.8239   Accuracy: 0.763\n",
      "Train   Epoch: 025 / 200   Loss:  0.2368   Accuracy: 0.917\n",
      " Test   Epoch: 025 / 200   Loss:   0.868   Accuracy: 0.766\n",
      "Train   Epoch: 026 / 200   Loss:  0.2057   Accuracy: 0.928\n",
      " Test   Epoch: 026 / 200   Loss:  0.8963   Accuracy: 0.764\n",
      "Train   Epoch: 027 / 200   Loss:  0.1797   Accuracy: 0.935\n",
      " Test   Epoch: 027 / 200   Loss:  0.9967   Accuracy: 0.760\n",
      "Train   Epoch: 028 / 200   Loss:  0.1611   Accuracy: 0.943\n",
      " Test   Epoch: 028 / 200   Loss:   1.055   Accuracy: 0.758\n",
      "Train   Epoch: 029 / 200   Loss:  0.1483   Accuracy: 0.946\n",
      " Test   Epoch: 029 / 200   Loss:   1.055   Accuracy: 0.761\n",
      "Train   Epoch: 030 / 200   Loss:  0.1262   Accuracy: 0.955\n",
      " Test   Epoch: 030 / 200   Loss:   1.187   Accuracy: 0.753\n",
      "Train   Epoch: 031 / 200   Loss:  0.1116   Accuracy: 0.960\n",
      " Test   Epoch: 031 / 200   Loss:   1.142   Accuracy: 0.762\n",
      "Train   Epoch: 032 / 200   Loss:  0.1015   Accuracy: 0.964\n",
      " Test   Epoch: 032 / 200   Loss:   1.223   Accuracy: 0.759\n",
      "Train   Epoch: 033 / 200   Loss: 0.09073   Accuracy: 0.968\n",
      " Test   Epoch: 033 / 200   Loss:   1.275   Accuracy: 0.762\n",
      "Train   Epoch: 034 / 200   Loss: 0.07281   Accuracy: 0.974\n",
      " Test   Epoch: 034 / 200   Loss:   1.437   Accuracy: 0.762\n",
      "Train   Epoch: 035 / 200   Loss: 0.07089   Accuracy: 0.975\n",
      " Test   Epoch: 035 / 200   Loss:   1.441   Accuracy: 0.761\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Define your model\n",
    "model = ConvNet(in_channels=3, num_classes=10)\n",
    "\n",
    "# Transfer it to device\n",
    "model = model.to(device=device)\n",
    "\n",
    "# Set a criterion (loss function)\n",
    "criterion = cross_entropy_scratch\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 200\n",
    "lr = 0.002\n",
    "momentum = 0.9\n",
    "\n",
    "# Train your model\n",
    "optimizer = MomentumSGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "train_loop_scratch(model=model,\n",
    "                   criterion=criterion,\n",
    "                   optimizer=optimizer,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   device=device,\n",
    "                   epochs=epochs,\n",
    "                   rounds = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi8OtKKXYWNF"
   },
   "source": [
    "# (D) Define and Train Convolutional Neural Network Using Pytorch\n",
    "In this section, you will use Pytorch to build and train a neural network for the task of super resolution of a single image. \n",
    "\n",
    "Please open the notebook located in `zssr/main.ipynb` (in a different tab), and follow the instructions there. Save and close this notebook."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aihub_env",
   "language": "python",
   "name": "aihub_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

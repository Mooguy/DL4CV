{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "u3ku6Rrgxz-b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title ## Mount Your Google Drive\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#@markdown The next two cells are **magic** cells.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#@markdown Please run this cell and follow the steps printed after running it. Specifically, it will print a URL you should enter, follow the instructions there and paste the code in the textbox below (and type `Enter`).\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     10\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "#@title ## Mount Your Google Drive\n",
    "\n",
    "#@markdown The next two cells are **magic** cells.\n",
    "#@markdown They look like text cells, but they run code behind the scenes.\n",
    "#@markdown You can run them by either clicking on the ‚ñ∂Ô∏è button (to the left of the cell), or by clicking on the cell and typing `Ctrl+Enter` (or `Shift+Enter`).\n",
    "\n",
    "#@markdown Please run this cell and follow the steps printed after running it. Specifically, it will print a URL you should enter, follow the instructions there and paste the code in the textbox below (and type `Enter`).\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5iG-fCsoqvhu"
   },
   "outputs": [],
   "source": [
    "#@title ## Map Your Directory\n",
    "import os\n",
    "\n",
    "def check_assignment(assignment_dir, files_list):\n",
    "  files_in_dir = set(os.listdir(assignment_dir))\n",
    "  for fname in files_list:\n",
    "    if fname not in files_in_dir:\n",
    "      raise FileNotFoundError(f'could not find file: {fname} in assignment_dir')\n",
    "\n",
    "assignment_dest = \"/content/hw2\"\n",
    "assignment_dir = \"/content/gdrive/MyDrive/DL4CV/hw2\"  #@param{type:\"string\"}\n",
    "assignment_files = ['hw2.ipynb', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n",
    "                    'models.py', 'models_torch.py', 'train.py', 'train_torch.py', 'utils.py',\n",
    "                    'test_autograd.py', 'test_functional.py', 'test_nn.py', 'test_optim.py']\n",
    "\n",
    "# check Google Drive is mounted\n",
    "if not os.path.isdir(\"/content/gdrive\"):\n",
    "  raise FileNotFoundError(\"Your Google Drive isn't mounted. Please run the above cell.\")\n",
    "\n",
    "# check all files there\n",
    "check_assignment(assignment_dir, assignment_files)\n",
    "\n",
    "# create symbolic link\n",
    "!rm -f {assignment_dest}\n",
    "!ln -s \"{assignment_dir}\" \"{assignment_dest}\"\n",
    "print(f'Succesfully mapped (ln -s) \"{assignment_dest}\" -> \"{assignment_dir}\"')\n",
    "\n",
    "# cd to linked dir\n",
    "%cd -q {assignment_dest}\n",
    "print(f'Succesfully changed directory (cd) to \"{assignment_dest}\"')\n",
    "#@markdown Set the path `assignment_dir` to the assignment directory in your Google Drive and run this cell.\n",
    "\n",
    "#@markdown If you are not sure what is the path, you can use the **Files (üìÅ)** menu (on the left side) to check the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIfpUXsh9qmW"
   },
   "source": [
    "## Imports and `autoreload`-Magic\n",
    "Please run the cell below (only once) to load and set the `autoreload` magic, which automatically reloads the import calls to the python files with your solutions. That means that you can edit the files (in the right-side window), save them (`Ctrl+S`) and just re-run the relevant cells -- the new code will kick in automatically.\n",
    "\n",
    "**Note:** You **MUST NOT** install any package. If you can't load something, you probably didn't follow the instructions (either didn't uploaded all the files, didn't mounted your Google driver or didn't mapped your directory).\n",
    "\n",
    "**Note:** The exercise works as is. If you add or modify imports to things, it may break thing in the notebook. You may do so **AT YOUR OWN RISK**. We will not assist with issues in notebook with modified imports.\n",
    "\n",
    "**Note:** Make sure you run **all the cells** up to the point. Some cells depends on previous cells (mainly imports). Furthermore, make sure to run the cell below (with the autoreload magic) before any cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x_Xg1LYZlnQ9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/home/labs/antebilab/guyilan/Courses/DL4CV/hw2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rwfbzEQnoXr"
   },
   "source": [
    "# (A) Implement Components for Deep Neural Network From Scratch\n",
    "\n",
    "In This part you will implement a deep neural network from scratch, including the necessary building blocks. You will implement it in the following order:\n",
    "\n",
    "1. **Differentiable Functions:** a set of differentiable functions that are used as atomic building blocks.\n",
    "2. **Autograd's backward:** the back-propagation `backward` method.\n",
    "3. **Learnable Layers:** the Linear layer.\n",
    "4. **Optimizer:** the SGD optimizer which will be used for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u32VOx2k3Pb8"
   },
   "source": [
    "## (A.1) Differentiable Functions\n",
    "\n",
    "In this section you will implement a set of differentiable functions from scratch. For each function, you will implement the forward and backward methods. After the description of the method, there is a testing cell which we will test the correctness of your code.\n",
    "\n",
    "The skeletons of the differential functions to implement are in the `functional.py` file. Open this file by clicking on this link: `/content/hw2/functional.py`. Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw2` (or `content/hw2`) and double-click on `functional.py` to open it. The tests can be found in `test_functional.py` (link: `/content/hw2/test_functional.py`).\n",
    "\n",
    "In each step you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n",
    "\n",
    "### `ctx`\n",
    "In the \"from scratch\" implementation, you should use a `ctx` (context) variable. This variable is a simplified version of the computation graph, and is needed for the back propagation algorithm.\n",
    "\n",
    "Specifically, `ctx` is just a list (or stack) of \"backward calls\", where each \"backward call\" is a pair (list/tuple) of two objects:\n",
    "\n",
    "1. **`backward_fn`:** The backward function. A reference to the backward function to be called in the backward pass.\n",
    "2. **`args`:** A list (or tuple) of arguments to be passed to `backward_fn`. This list usually consists of the inputs and the outputs of the forward function. Sometimes additional arguments are passed as well. It's important to pass the actual inputs and outputs (same pointer), otherwise it would break the chain of gradients propagation.\n",
    "\n",
    "The \"backward calls\" in `ctx` should be ordered in according to the time of addition. That is, a backward call that was added later should have an higher index in the list `ctx`. If `ctx` is `None`, it means that gradients (i.e. backward calls) should not be tracked.\n",
    "\n",
    "You will use `ctx` in the backward pass in section (A.2). You can read it now to get a little context (pun intended).\n",
    "\n",
    "**Note:** You are given an example of the forward and backward implementation of `mean`. You should read and understand how new backward calls are appended to `ctx`, and use this pattern in your solutions.\n",
    "\n",
    "**Note:** When new tensors are created (using `zeros`, `ones`, `rand`, etc.), it's important to make sure they are on the same device (and has the correct `dtype`) as tensors they would be used together with (compared to, multiplied by, etc.). You may find the functions `torch.X_like` and `Tensor.new_X` handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRhfmjXODEo"
   },
   "source": [
    "### (A.1.1) Implement the Linear Function\n",
    "\n",
    "Here you will implement a differentiable `linear` function. This includes the forward `linear` function and the backward `linear_backward` function.\n",
    "\n",
    "#### `linear`\n",
    "The `linear` function receives three arguments (in addition to the autograd context `ctx`):\n",
    "\n",
    "  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n",
    "  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n",
    "  * `b`: The bias term. Has shape `(out_dim,)`.\n",
    "\n",
    "It computes the (batched version of the) function: $$ \\mathbf{y} = W \\mathbf{x} + \\mathbf{b} $$\n",
    "The output `y` should have shape `(batch_size, out_dim)`.\n",
    "\n",
    "#### `linear_backward`\n",
    "The `linear_backward` function receives four arguments:\n",
    "\n",
    "  * `y`: The batched output. Has shape `(batch_size, out_dim)`.\n",
    "  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n",
    "  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n",
    "  * `b`: The bias term. Has shape `(out_dim,)`.\n",
    "\n",
    "It computes the gradients of `x`, `w` and `b` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates these gradients in `x.grad`, `w.grad` and `b.grad`, respectively.\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OkrZhq0U31I0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.022s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UAqdVJl9uxrI"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging linear\n",
    "from functional import linear, linear_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3vGFltjkDSm"
   },
   "source": [
    "### (A.1.2) Implement the ReLU Activation\n",
    "\n",
    "Here you will implement a differentiable `relu` activation. This includes the forward `relu` function and the backward `relu_backward` function.\n",
    "\n",
    "#### `relu`\n",
    "The `relu` function receives one argument (in addition to the autograd context `ctx`):\n",
    "\n",
    "  * `x`: The input. Has an arbitrary shape.\n",
    "\n",
    "It computes the (element-wise) function:\n",
    "$$ y = \\max(x, 0) $$\n",
    "The output `y` should have the same shape as `x`.\n",
    "\n",
    "#### `relu_backward`\n",
    "The `relu_backward` function receives two arguments:\n",
    "\n",
    "  * `y`: The output. Has the same shape as `x`.\n",
    "  * `x`: The input. Has an arbitrary shape.\n",
    "\n",
    "It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lA-NX0Y-j8yF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.022s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m_79D4JGj888"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging relu\n",
    "from functional import relu, relu_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKtM0ZFakD3q"
   },
   "source": [
    "### (A.1.3) Implement the Softmax Activation\n",
    "\n",
    "Here you will implement a differentiable `softmax` activation. This includes the forward `softmax` function and the backward `softmax_backward` function.\n",
    "\n",
    "**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n",
    "\n",
    "#### `softmax`\n",
    "The `softmax` function receives one argument (in addition to the autograd context `ctx`):\n",
    "\n",
    "  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n",
    "\n",
    "It computes the (batched version of the) function: $$ \\mathbf{y}_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j{e^{\\mathbf{x}_j}}} $$\n",
    "The output `y` should have the shape `(batch_size, num_classes)`. Each row in `y` should be a probability distribution over the classes.\n",
    "\n",
    "\n",
    "#### `softmax_backward`\n",
    "The `softmax_backward` function receives two arguments:\n",
    "\n",
    "  * `y`: The batched output. Has shape `(batch_size, num_classes)`.\n",
    "  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n",
    "\n",
    "It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AqSS59Pej9FY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.076s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jHN0ZW5Xj9Lv"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging softmax\n",
    "from functional import softmax, softmax_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL1_-cE5kEiG"
   },
   "source": [
    "### (A.1.4) Implement the Cross-Entropy Loss\n",
    "\n",
    "Here you will implement a differentiable `cross_entropy` activation. This includes the forward `cross_entropy` function and the backward `cross_entropy_backward` function.\n",
    "\n",
    "**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n",
    "\n",
    "**Note:** The signature of this function differs from PyTorch's `F.cross_entropy`. The function you should implement doesn't \"reduce\" (i.e. averages over) the batch (similarly to `F.cross_entropy(..., reduction='none')`). Furthermore, while `F.cross_entropy` receives the predictions **before** `softmax`, the function you should implement receives the predictions **after** `softmax`. We provide you the `cross_entropy_loss` which uses your implementation of `softmax` and `cross_entropy`, and has the same API as `F.cross_entropy`.\n",
    "\n",
    "#### `cross_entropy`\n",
    "The `cross_entropy` function receives two arguments (in addition to the autograd context `ctx`):\n",
    "\n",
    "  * `pred`: The predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n",
    "  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n",
    "\n",
    "It computes the (batched version of the) function:\n",
    "$$ \\text{CE}(\\hat{\\mathbf{y}}, \\ell)_i = -\\log(\\hat{\\mathbf{y}}_i) \\cdot \\delta_{i,\\ell} $$\n",
    "Where $\\hat{\\mathbf{y}}$ (also called `pred` or `y_hat`) is the predicted probability measure over the classes and $\\ell$ (also called `target` or `y`) is the target class label.\n",
    "\n",
    "The output `loss` should have the shape `(batch_size,)`. Each row in `loss` should be the cross-entropy loss of that entry in the batch.\n",
    "\n",
    "#### `cross_entropy_backward`\n",
    "The `cross_entropy_backward` function receives three arguments:\n",
    "\n",
    "  * `loss`: The batched loss. Has shape `(batch_size,)`.\n",
    "  * `pred`: The batched predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n",
    "  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n",
    "\n",
    "It computes the gradients of `pred` w.r.t the (final scalar) loss, given the gradient of (batched) `loss` (in `loss.grad`) w.r.t the loss, and accumulates this gradient in `pred.grad`.\n",
    "\n",
    "#### `cross_entropy_loss`\n",
    "This function is provided for your use. It calls `softmax` to compute the probability distribution over the labels, then `cross_entropy` to computed the batched loss, and later `mean` to reduce it into a scalar loss (that can be used as the origin of gradients; see next part). You should NOT modify this method, and may use it later on.\n",
    "\n",
    "**Note:** Please see how three differentiable functions (`softmax`, `cross_entropy` and `mean`) are chained to create a new differentiable function, without explicitly implementing its backward pass. You will chain differentiable functions to create a model in section (B).\n",
    "\n",
    "---\n",
    "You should test your solution by running the following cell. You can debug your solution in the cell below it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3jW24PjRj9RE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.090s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_functional.TestCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S6au2xEbj9Vu"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging cross_entropy\n",
    "from functional import cross_entropy, cross_entropy_backward\n",
    "from functional import cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e_g4fDakFoQ"
   },
   "source": [
    "## (A.2) Autograd\n",
    "\n",
    "In this section you will implement a general `backward` method from scratch. This method stands at the core of back-propagation and autograd differentiation.\n",
    "\n",
    "This method receives two arguments:\n",
    "\n",
    "* `loss`: The loss tensor. This tensor must be a scalar (Has shape `()`). The loss the other tensors will be computed w.r.t this `loss`.\n",
    "* `ctx`: The autograd context. A list of backward calls. These backward calls should be evaluated to back-propagate the gradient from `loss` to the tensors used in the computation of `loss`.\n",
    "\n",
    "This method has two main steps:\n",
    "\n",
    "* Setting the gradient of `loss` (to what?).\n",
    "* Propagating the gradients backward using the computation history in `ctx` (how?).\n",
    "\n",
    "The skeleton of the `backward` method is in the `autograd.py` file (link: `/content/hw2/autograd.py`). The tests can be found in `test_autograd.py` (link: `/content/hw2/test_autograd.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. You can use the provided `create_grad_if_necessary` which makes sure that tensors that need gradients have one (if not, it creates a `.grad` attribute in the tensor's shape filled with zeros). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bTb1J0bFj9ag"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.023s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_autograd.TestBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "veWDHxTTj9fl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward\n",
      "loss.grad: None\n",
      "y.grad: None\n",
      "x.grad: None\n",
      "\n",
      "\n",
      "after backward\n",
      "loss.grad: tensor(1.)\n",
      "y.grad: tensor([[ 0.0873,  0.0418,  0.0406, -0.2103,  0.0407],\n",
      "        [ 0.0689,  0.0504, -0.2054,  0.0426,  0.0435],\n",
      "        [ 0.0524,  0.0441,  0.0412,  0.0591, -0.1969],\n",
      "        [ 0.0548,  0.0537,  0.0514, -0.2056,  0.0457]])\n",
      "x.grad: tensor([[ 0.0117, -0.0023, -0.0015, -0.0064, -0.0015],\n",
      "        [ 0.0191,  0.0038, -0.0243,  0.0005,  0.0008],\n",
      "        [ 0.0167,  0.0048,  0.0008,  0.0271, -0.0494],\n",
      "        [ 0.0071,  0.0063,  0.0047, -0.0198,  0.0017]])\n"
     ]
    }
   ],
   "source": [
    "# Playground for debugging backward\n",
    "from autograd import backward\n",
    "from functional import cross_entropy_loss, softmax\n",
    "\n",
    "# You CAN modify the content of the cell below. It is just an example.\n",
    "ctx = []\n",
    "x = torch.randn(4, 5)\n",
    "l = torch.randint(5, size=(4,), dtype=torch.long)\n",
    "y = softmax(x, ctx=ctx)\n",
    "loss = cross_entropy_loss(y, l, ctx=ctx)\n",
    "\n",
    "print('before backward')\n",
    "print('loss.grad:', loss.grad)\n",
    "print('y.grad:', y.grad)\n",
    "print('x.grad:', x.grad)\n",
    "\n",
    "backward(loss, ctx)\n",
    "\n",
    "print('\\n\\nafter backward')\n",
    "print('loss.grad:', loss.grad)\n",
    "print('y.grad:', y.grad)\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F30SfhnmkWP"
   },
   "source": [
    "## (A.3) Learnable Layers\n",
    "\n",
    "In this section you will implement a learnable Linear layer. The implementation is similar to vanilla PyTorch.\n",
    "\n",
    "The skeleton of the learnable Linear layer to implement is in the `nn.py` file (link: `/content/hw2/nn.py`). The tests can be found in `test_nn.py` (link: `/content/hw2/test_nn.py`).\n",
    "\n",
    "Learnable layers (and networks) inherits from the provided class `Module` (which is similar to PyTorch's `nn.Module`). This abstract class implements some utility methods (some are not used in this assignment). Please read the list of `Module`'s methods and attributes in its documentation (link: `/content/hw2/nn.py`).\n",
    "\n",
    "In the `nn.py` file, you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n",
    "\n",
    "**Note:** To see how \"atomic\" differentiable functions are composed into a complex differentiable function, please look at the provided `cross_entropy_loss` in `/content/hw2/functional.py`.\n",
    "\n",
    "**Note:** Since this part doesn't use PyTorch's built-in autograd mechanism, please do not use tensors' `requires_grad` (this will result in errors/warnings).\n",
    "Furthermore, do not use `nn.Parameter` in _from scratch_ layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkxNHgUnAoa"
   },
   "source": [
    "### (A.3.1) Implement the Linear Layer\n",
    "\n",
    "So far you have implemented *stateless* differentiable functions, and the autograd mechanism. In this section, you will implement a *stateful* layer, with learnable parameters. That is the `Linear` layer.\n",
    "\n",
    "The parameters of the `Linear` layer are the weight matrix `weight` and the bias term `bias`. In your layer, you should:\n",
    "\n",
    "1. **Create parameter tensors:** create tensors for the parameters in the correct shape. The parameters should be attributes of the layer, i.e. set as `self.<param> = <tensor>`. This is done in `Linear.__init__`.\n",
    "2. **Register them as parameters:** add their names to `self._parameters`. This will be used by the provided `Module.parameters()` (to list module's parameters) and `Module.to()` (to trasfer module's parameters to a device) methods. This is done in `Linear.__init__`.\n",
    "3. **Initialize the parameters:** initialization of the layer parameters has significant influence on the local minimum the network reaches during training. This is done in `Linear.init_parameters()`. You should call this method from `Linear.__init__`, so newly created linear layers are initialized.\n",
    "4. **Implement a forward method:** use the existing differentiable function from part A, and implement the `Linear.forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ARMFodrXnBfW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.089s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_nn.TestLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTKP7Ovas-_M"
   },
   "source": [
    "## (A.4) Optimizer\n",
    "\n",
    "In this section you will implement an optimizer. The optimizer updates the parameters based on the gradients they had accumulated. To do so it should have three main functions:\n",
    "\n",
    "1. `__init__`: Receives the list of parameters (weights) to update their values and save them. May receive additional arguments, such as learning-rate, etc.\n",
    "2. `step`: Updates the parameters values based on the value of their gradients. Doesn't receive any argument.\n",
    "3. `zero_grad`: Zeros the gradients of the tracked parameters. This is necessary since gradients are accumulated in each backward pass, and we don't want to mix between batches. Doesn't receive any argument.\n",
    "\n",
    "The skeleton of the optimizer is in the `optim.py` file (link: `/content/hw2/optim.py`). The tests can be found in `test_optim.py` (link: `/content/hw2/test_optim.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK5kaf5kmoxM"
   },
   "source": [
    "### (A.4.1) SGD Optimizer\n",
    "In this part, you'll implement an SGD optimizer. This optimizer has a simple update rule, which is:\n",
    "$$\\mathbf{x}_{n+1} = \\mathbf{x}_{n} - \\delta \\cdot \\mathbf{g}_{n} $$\n",
    "Where $\\mathbf{x}_{n}$ is the parameter at step $n$, $\\mathbf{g}_{n}$ is its gradient at step $n$, and $\\delta$ is the learning rate (also called `lr`).\n",
    "\n",
    "You should implement the `__init__`, `step` and `zero_grad` methods of `SGD` optimizer in `optim.py`.\n",
    "\n",
    "**Note:** Parameters (tensors) should be updated **in-place** (i.e. with the `-=` operator) in `step`.\n",
    "\n",
    "**Note:** A gradient (`param.grad`) which is set to `None` is also considered as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_optim.TestSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dZlZwEqGj9_2"
   },
   "outputs": [],
   "source": [
    "# Playground for debugging SGD\n",
    "from optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XStdhdlPcSBd"
   },
   "source": [
    "# Setup Before Training\n",
    "\n",
    "In this part you will need to use GPU (this will have a significant impact on the training speed). To get a GPU in Google Colab, please go to the top menu and to: **Runtime ‚ûî Change runtime type**. Then, select **GPU** as **Hardware accelerator**.\n",
    "\n",
    "Please run the cell below to set your pytorch device (either GPU or CPU), to load the dataset and to create data loaders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cVRcw-dlgFLb"
   },
   "outputs": [],
   "source": [
    "from utils import load_mnist\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pin_memory = device.type == 'cuda'\n",
    "\n",
    "# Load the training and test sets\n",
    "train_data = load_mnist(mode='train')\n",
    "test_data = load_mnist(mode='test')\n",
    "\n",
    "# Create dataloaders for training and test sets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiJaEoARnAOn"
   },
   "source": [
    "# (B) Define and Train Neural Networks From Scratch\n",
    "\n",
    "\n",
    "In this part, you will define and train neural networks from scratch. You will use your differentiable functions from section (A).\n",
    "\n",
    "The skeletons for this assignment can be found in the `models.py` (link: `/content/hw2/models.py`) and `train.py` (link: `/content/hw2/train.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n",
    "\n",
    "Please run the cell below to import the relevant objects in order to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PzTMHw9yKxN6"
   },
   "outputs": [],
   "source": [
    "from functional import cross_entropy_loss as cross_entropy_scratch\n",
    "from models import SoftmaxClassifier as SoftmaxClassifierScratch\n",
    "from models import MLP as MLPScratch\n",
    "from optim import SGD as SGDScratch\n",
    "from train import train_loop as train_loop_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW255-xmnA6O"
   },
   "source": [
    "## (B.1) Implement and Train a SoftmaxClassifier\n",
    "\n",
    "Here you will implement the `SoftmaxClassifier` (imported here as `SoftmaxClassifierScratch`). You have already implemented the `SoftmaxClassifier` in Homework 1, but now it will be implemented with autograd and modular differentiable functions.\n",
    "\n",
    "Your solution should have the following parts:\n",
    "\n",
    "1. Create a model.\n",
    "2. (Optional) Transfer the model to `device`.\n",
    "3. Create an optimizer. (this should be done when the model is in its final device. It will not work otherwise).\n",
    "4. Set other hyper-parameters (loss function, number of epochs, etc.).\n",
    "5. Train the model.\n",
    "\n",
    "**Note:** As opposed to its name, `SoftmaxClassifier` should not perform softmax. That's because softmax part of the cross-entropy loss (in PyTorch and in the _from scratch_ section).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EQlsopQc3dF5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 010   Loss:  0.9582   Accuracy: 0.797\n",
      " Test   Epoch: 001 / 010   Loss:  0.5821   Accuracy: 0.868\n",
      "Train   Epoch: 002 / 010   Loss:  0.5345   Accuracy: 0.870\n",
      " Test   Epoch: 002 / 010   Loss:  0.4629   Accuracy: 0.885\n",
      "Train   Epoch: 003 / 010   Loss:  0.4584   Accuracy: 0.881\n",
      " Test   Epoch: 003 / 010   Loss:  0.4154   Accuracy: 0.892\n",
      "Train   Epoch: 004 / 010   Loss:  0.4216   Accuracy: 0.887\n",
      " Test   Epoch: 004 / 010   Loss:  0.3889   Accuracy: 0.897\n",
      "Train   Epoch: 005 / 010   Loss:  0.3989   Accuracy: 0.891\n",
      " Test   Epoch: 005 / 010   Loss:  0.3707   Accuracy: 0.901\n",
      "Train   Epoch: 006 / 010   Loss:  0.3831   Accuracy: 0.895\n",
      " Test   Epoch: 006 / 010   Loss:   0.358   Accuracy: 0.903\n",
      "Train   Epoch: 007 / 010   Loss:  0.3712   Accuracy: 0.897\n",
      " Test   Epoch: 007 / 010   Loss:  0.3486   Accuracy: 0.906\n",
      "Train   Epoch: 008 / 010   Loss:  0.3619   Accuracy: 0.899\n",
      " Test   Epoch: 008 / 010   Loss:  0.3411   Accuracy: 0.906\n",
      "Train   Epoch: 009 / 010   Loss:  0.3544   Accuracy: 0.901\n",
      " Test   Epoch: 009 / 010   Loss:  0.3351   Accuracy: 0.907\n",
      "Train   Epoch: 010 / 010   Loss:  0.3481   Accuracy: 0.902\n",
      " Test   Epoch: 010 / 010   Loss:  0.3297   Accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Define your model\n",
    "model = SoftmaxClassifierScratch(784, 10)\n",
    "\n",
    "# Transfer it to device\n",
    "model.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = SGDScratch(model.parameters(), lr=1e-3) \n",
    "\n",
    "# Set a criterion (loss function)\n",
    "criterion = cross_entropy_scratch\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Train your model\n",
    "train_loop_scratch(model=model,\n",
    "                   criterion=criterion,\n",
    "                   optimizer=optimizer,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   device=device,\n",
    "                   epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNtYgm9vnGtR"
   },
   "source": [
    "## (B.2) Implement and Train a Deep Neural Network\n",
    "\n",
    "Here you will implement a multi-layer perceptron (`MLP`) model (imported here as `MLPScratch`). You are allowed to modify the signiture of `MLP.__init__` and add additional arguments to your choice. Your network must have more than a single linear layer.\n",
    "\n",
    "Your solution should have the same parts as in (B.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVU5EM6CSgHL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 010   Loss:   2.203   Accuracy: 0.478\n",
      " Test   Epoch: 001 / 010   Loss:   2.106   Accuracy: 0.522\n",
      "Train   Epoch: 002 / 010   Loss:   2.028   Accuracy: 0.557\n",
      " Test   Epoch: 002 / 010   Loss:   1.943   Accuracy: 0.589\n",
      "Train   Epoch: 003 / 010   Loss:   1.883   Accuracy: 0.599\n",
      " Test   Epoch: 003 / 010   Loss:   1.808   Accuracy: 0.626\n",
      "Train   Epoch: 004 / 010   Loss:   1.762   Accuracy: 0.626\n",
      " Test   Epoch: 004 / 010   Loss:   1.696   Accuracy: 0.653\n",
      "Train   Epoch: 005 / 010   Loss:    1.66   Accuracy: 0.648\n",
      " Test   Epoch: 005 / 010   Loss:   1.602   Accuracy: 0.669\n",
      "Train   Epoch: 006 / 010   Loss:   1.575   Accuracy: 0.663\n",
      " Test   Epoch: 006 / 010   Loss:   1.522   Accuracy: 0.685\n",
      "Train   Epoch: 007 / 010   Loss:   1.503   Accuracy: 0.677\n",
      " Test   Epoch: 007 / 010   Loss:   1.454   Accuracy: 0.696\n",
      "Train   Epoch: 008 / 010   Loss:    1.44   Accuracy: 0.687\n",
      " Test   Epoch: 008 / 010   Loss:   1.395   Accuracy: 0.702\n",
      "Train   Epoch: 009 / 010   Loss:   1.386   Accuracy: 0.695\n",
      " Test   Epoch: 009 / 010   Loss:   1.344   Accuracy: 0.711\n",
      "Train   Epoch: 010 / 010   Loss:   1.339   Accuracy: 0.703\n",
      " Test   Epoch: 010 / 010   Loss:   1.299   Accuracy: 0.718\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "model = MLPScratch(784, 10, hidden_dim=64)\n",
    "\n",
    "# Transfer it to device\n",
    "model.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = SGDScratch(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Set a criterion (loss function)\n",
    "criterion = cross_entropy_scratch\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Train your model\n",
    "train_loop_scratch(model=model,\n",
    "                   criterion=criterion,\n",
    "                   optimizer=optimizer,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   device=device,\n",
    "                   epochs=epochs)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PSeMqMC3fBc"
   },
   "source": [
    "# (C) Define and Train PyTorch Neural Networks\n",
    "\n",
    "In this part, you will define and train neural networks using PyTorch's built-in autograd mechanism. You MAY NOT use your differentiable functions from section (A). The solution to this part is very similar to the solution of part (B), with some syntax changes.\n",
    "\n",
    "The skeletons for this assignment can be found in the `models_torch.py` (link: `/content/hw2/models_torch.py`) and `train_torch.py` (link: `/content/hw2/train_torch.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n",
    "\n",
    "Please run the cell below to import the relevant objects in order to train the models.\n",
    "\n",
    "**Note:** some methods are imported with different names in this notebook to distinguish them from the _From Scratch_ part. This is not a best practice, and used solely as a way to avoid ambiguities in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1eMq2E6F3kwR"
   },
   "outputs": [],
   "source": [
    "# NOTE: `cross_entropy_torch` is different from `cross_entropy_scratch`!\n",
    "# cross_entropy_torch(pred, target) == cross_entropy_scratch(softmax(pred), target)\n",
    "from torch.nn.functional import cross_entropy as cross_entropy_torch\n",
    "from models_torch import SoftmaxClassifier as SoftmaxClassifierTorch\n",
    "from models_torch import MLP as MLPTorch\n",
    "from torch.optim import SGD as SGDTorch\n",
    "from train_torch import train_loop as train_loop_torch\n",
    "from utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFI-VKr73bcE"
   },
   "source": [
    "## (C.1) Implement and Train a Softmax Classifier\n",
    "\n",
    "Here you will implement the `SoftmaxClassifier` class (imported as `SoftmaxClassifierTorch`).\n",
    "\n",
    "Your solution should have the same parts as in (B.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "e0U2HGz13b0W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 010   Loss:  0.9651   Accuracy: 0.762\n",
      " Test   Epoch: 001 / 010   Loss:  0.5851   Accuracy: 0.862\n",
      "Train   Epoch: 002 / 010   Loss:  0.5386   Accuracy: 0.865\n",
      " Test   Epoch: 002 / 010   Loss:  0.4651   Accuracy: 0.883\n",
      "Train   Epoch: 003 / 010   Loss:  0.4619   Accuracy: 0.877\n",
      " Test   Epoch: 003 / 010   Loss:  0.4172   Accuracy: 0.890\n",
      "Train   Epoch: 004 / 010   Loss:  0.4248   Accuracy: 0.885\n",
      " Test   Epoch: 004 / 010   Loss:    0.39   Accuracy: 0.894\n",
      "Train   Epoch: 005 / 010   Loss:  0.4019   Accuracy: 0.889\n",
      " Test   Epoch: 005 / 010   Loss:  0.3726   Accuracy: 0.898\n",
      "Train   Epoch: 006 / 010   Loss:  0.3859   Accuracy: 0.894\n",
      " Test   Epoch: 006 / 010   Loss:  0.3604   Accuracy: 0.900\n",
      "Train   Epoch: 007 / 010   Loss:   0.374   Accuracy: 0.896\n",
      " Test   Epoch: 007 / 010   Loss:  0.3505   Accuracy: 0.903\n",
      "Train   Epoch: 008 / 010   Loss:  0.3646   Accuracy: 0.899\n",
      " Test   Epoch: 008 / 010   Loss:  0.3424   Accuracy: 0.905\n",
      "Train   Epoch: 009 / 010   Loss:  0.3569   Accuracy: 0.900\n",
      " Test   Epoch: 009 / 010   Loss:  0.3362   Accuracy: 0.905\n",
      "Train   Epoch: 010 / 010   Loss:  0.3504   Accuracy: 0.901\n",
      " Test   Epoch: 010 / 010   Loss:  0.3311   Accuracy: 0.906\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Define your model\n",
    "model = SoftmaxClassifierTorch(784, 10)\n",
    "\n",
    "# Transfer it to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = SGDTorch(model.parameters(), lr=1e-3) \n",
    "\n",
    "# Set a criterion (loss function)\n",
    "criterion = cross_entropy_torch\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Train your model\n",
    "train_loop_torch(model=model,\n",
    "                 criterion=criterion,\n",
    "                 optimizer=optimizer,\n",
    "                 train_loader=train_loader,\n",
    "                 test_loader=test_loader,\n",
    "                 device=device,\n",
    "                 epochs=epochs)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc3NLJiwXsuH"
   },
   "source": [
    "## (C.2) Implement and Train a Deep Neural Network\n",
    "\n",
    "Here you will implement the `MLP` class (imported as `MLPTorch`).\n",
    "\n",
    "Your solution should have the same parts as in (B.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JbXH5Dt4LVHu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 010   Loss:   2.173   Accuracy: 0.325\n",
      " Test   Epoch: 001 / 010   Loss:   1.965   Accuracy: 0.552\n",
      "Train   Epoch: 002 / 010   Loss:   1.635   Accuracy: 0.685\n",
      " Test   Epoch: 002 / 010   Loss:   1.249   Accuracy: 0.757\n",
      "Train   Epoch: 003 / 010   Loss:       1   Accuracy: 0.780\n",
      " Test   Epoch: 003 / 010   Loss:   0.773   Accuracy: 0.820\n",
      "Train   Epoch: 004 / 010   Loss:  0.6884   Accuracy: 0.831\n",
      " Test   Epoch: 004 / 010   Loss:  0.5852   Accuracy: 0.852\n",
      "Train   Epoch: 005 / 010   Loss:  0.5553   Accuracy: 0.855\n",
      " Test   Epoch: 005 / 010   Loss:  0.4961   Accuracy: 0.868\n",
      "Train   Epoch: 006 / 010   Loss:  0.4852   Accuracy: 0.868\n",
      " Test   Epoch: 006 / 010   Loss:  0.4436   Accuracy: 0.879\n",
      "Train   Epoch: 007 / 010   Loss:  0.4416   Accuracy: 0.877\n",
      " Test   Epoch: 007 / 010   Loss:  0.4088   Accuracy: 0.886\n",
      "Train   Epoch: 008 / 010   Loss:  0.4114   Accuracy: 0.884\n",
      " Test   Epoch: 008 / 010   Loss:  0.3846   Accuracy: 0.892\n",
      "Train   Epoch: 009 / 010   Loss:  0.3891   Accuracy: 0.890\n",
      " Test   Epoch: 009 / 010   Loss:  0.3654   Accuracy: 0.895\n",
      "Train   Epoch: 010 / 010   Loss:  0.3716   Accuracy: 0.894\n",
      " Test   Epoch: 010 / 010   Loss:   0.351   Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Define your model\n",
    "model = MLPTorch(784, 10)\n",
    "\n",
    "# Transfer it to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = SGDTorch(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Set a criterion (loss function)\n",
    "criterion = cross_entropy_torch \n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Train your model\n",
    "train_loop_torch(model=model,\n",
    "                 criterion=criterion,\n",
    "                 optimizer=optimizer,\n",
    "                 train_loader=train_loader,\n",
    "                 test_loader=test_loader,\n",
    "                 device=device,\n",
    "                 epochs=epochs)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxim_oNddkqE"
   },
   "source": [
    "# Submit Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "id": "5OjcF__gXmth"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_zip\u001b[39m(files, hw, name):\n\u001b[1;32m      9\u001b[0m   zip_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "#@title # Create and Download Your Solution\n",
    "\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def create_zip(files, hw, name):\n",
    "  zip_path = f'{hw}-{name}.zip'\n",
    "  with zipfile.ZipFile(zip_path, 'w') as f:\n",
    "    for fname in files:\n",
    "      if not os.path.isfile(fname):\n",
    "        raise FileNotFoundError(f\"Couldn't find file: '{fname}' in the homework directory\")\n",
    "      f.write(fname, fname)\n",
    "  return zip_path\n",
    "\n",
    "# export notebook as html\n",
    "!jupyter nbconvert --to html hw2.ipynb\n",
    "\n",
    "##@markdown Please upload your typed solution (`.pdf` file) to the homework directory, and use the name `hw2-sol.pdf`.\n",
    "\n",
    "student_name = \"John Doe\"  #@param{type:\"string\"}\n",
    "assignment_name = 'hw2'\n",
    "assignment_sol_files = ['hw2.ipynb', 'hw2.html', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n",
    "                        'models.py', 'models_torch.py', 'train.py', 'train_torch.py']\n",
    "zip_name = re.sub('[_ ]+', '_', re.sub(r'[^a-zA-Z_ ]+', '', student_name.lower()))\n",
    "\n",
    "# create zip with your solution\n",
    "zip_path = create_zip(assignment_sol_files, assignment_name, zip_name)\n",
    "\n",
    "# download the zip\n",
    "files.download(zip_path)\n",
    "\n",
    "#@markdown Enter your name in `student_name` and run this cell to create and download a `.zip` file with your solution.\n",
    "\n",
    "#@markdown You should submit your solution via Moodle.\n",
    "\n",
    "#@markdown **Note:** If you run this cell multiple times, you may be prompted by the browser to allow this page to download multiple files."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

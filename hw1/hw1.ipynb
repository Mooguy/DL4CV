{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "u3ku6Rrgxz-b"
   },
   "outputs": [],
   "source": [
    "#@title ## Mount Your Google Drive\n",
    "#@markdown Please run this cell (`Ctrl+Enter` or `Shift+Enter`) and follow the steps printed bellow.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RzNUFliwGRvy"
   },
   "outputs": [],
   "source": [
    "#@title ## Map Your Directory\n",
    "import os\n",
    "\n",
    "def check_assignment(assignment_dir, files_list):\n",
    "  files_in_dir = set(os.listdir(assignment_dir))\n",
    "  for fname in files_list:\n",
    "    if fname not in files_in_dir:\n",
    "      raise FileNotFoundError(f'could not find file: {fname} in assignment_dir')\n",
    "\n",
    "assignment_dest = \"/content/hw1\"\n",
    "assignment_dir = \"/content/gdrive/MyDrive/DL4CV/hw1\"  #@param{type:\"string\"}\n",
    "assignment_files = ['hw1.ipynb', 'model.py', 'test_model.py', 'train.py', 'utils.py']\n",
    "\n",
    "# check Google Drive is mounted\n",
    "if not os.path.isdir(\"/content/gdrive\"):\n",
    "  raise FileNotFoundError(\"Your Google Drive isn't mounted. Please run the above cell.\")\n",
    "\n",
    "# check all files there\n",
    "check_assignment(assignment_dir, assignment_files)\n",
    "\n",
    "# create symbolic link\n",
    "!rm -f {assignment_dest}\n",
    "!ln -s \"{assignment_dir}\" \"{assignment_dest}\"\n",
    "print(f'Succesfully mapped (ln -s) \"{assignment_dest}\" -> \"{assignment_dir}\"')\n",
    "\n",
    "# cd to linked dir\n",
    "%cd -q {assignment_dest}\n",
    "print(f'Succesfully changed directory (cd) to \"{assignment_dest}\"')\n",
    "#@markdown Set the path `assignment_dir` to the assignment directory in your Google Drive and run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pytuR2oP1v8t"
   },
   "outputs": [],
   "source": [
    "#@title ## Written Assignment\n",
    "\n",
    "#@markdown In addition to this coding assignment, there is also a written assignment that can be found in `hw1.pdf`.\n",
    "\n",
    "#@markdown Please solve this assignment and upload your solution as `hw1-sol.pdf`. It will be packed together with your coding solution in the **Submit Your Solution** section.\n",
    "\n",
    "#@markdown Your solution to the written part **should be typed**, not hand-written. We recommend using LyX or LaTex, but you can also use Word or similar text editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u32VOx2k3Pb8"
   },
   "source": [
    "# (A) Implement Softmax Classifier From Scratch\n",
    "\n",
    "In this section of the exercise, you will implement a Softmax Classifier step-by-step, from scratch.\n",
    "\n",
    "You should open the `model.py` file (by clicking on this link: `/content/hw1/model.py`). Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw1` (or `content/hw1`) and double-click on `model.py`.\n",
    "\n",
    "In each part you will be asked to implement a single method. Your solution should be between the `# BEGIN SOLUTION` and `# END SOLUTION` comments. You should also remove the `raise NotImplementedError` line in your solution.\n",
    "\n",
    "After the description of the method in this notebook, there is a testing cell which will test the correctness of your code (tests code is in: `/content/hw1/test_model.py`).\n",
    "\n",
    "**Note:** The files in this assignment are auto-imported in this notebook. It means that you can change them, save them (`Ctrl+S`) and this change will immediately take affect in the notebook (when you use these functions again). You can use the dedicated playground cells to debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H2hieYEAtHqD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRhfmjXODEo"
   },
   "source": [
    "## (A.1) Implement Softmax\n",
    "\n",
    "In this part you will implement the `softmax` activation function, which is defined as:\n",
    "$$ \\text{softmax}(\\mathbf{x})_i = \\frac{\\exp(\\mathbf{x}_i)}{\\sum_{j=1}^{n} \\exp(\\mathbf{x}_j)} $$\n",
    "\n",
    "The output of `softmax` is a probability measure over the `n` classes.\n",
    "\n",
    "Since the use of batches is very common in ML and DL, your implementation should support running `softmax` of a batch of vectors (_i.e._ a tensor of shape `(batch_size, n)`). The softmax function is applied to each vector in the batch _independently_.\n",
    "\n",
    "Real numbers have a fixed-length representation in computers, so very large numbers cannot be represented. **Your solution should be numerically stable.**\n",
    "\n",
    "To solve this part, please implement the `softmax` function in `model.py`. You can test your solution by running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OkrZhq0U31I0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.098s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_model.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UAqdVJl9uxrI"
   },
   "outputs": [],
   "source": [
    "# playground for debugging softmax\n",
    "from model import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPtqk2SmRVD0"
   },
   "source": [
    "## (A.2) Cross-Entropy Loss\n",
    "\n",
    "In this part you will implement the `cross_entropy` loss function (for hard-label), which is defined as:\n",
    "$$ \\text{CE}(\\hat{\\mathbf{y}}, \\ell)_i = -\\log(\\hat{\\mathbf{y}}_i) \\cdot \\delta_{i,\\ell} $$\n",
    "\n",
    "Where $\\hat{\\mathbf{y}}$ (also called `pred` or `y_hat`) is the predicted probability measure over the classes and $\\ell$ (also called `target` or `y`) is the target class label.\n",
    "\n",
    "As before, you are required to make sure that your solution should support batches and be numerically stable.\n",
    "\n",
    "To solve this part, please implement the `cross_entropy` function in `model.py`. You can test your solution by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1N98cH30B17b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.030s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_model.CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qX1DUnHm-h3"
   },
   "source": [
    "## (A.3) Softmax Classifer\n",
    "\n",
    "In this part you will implement the `softmax_classifier` function, which recieves an input $\\mathbf{x}$, a weight matrix $W$ and a bias term $\\mathbf{b}$ and returns:\n",
    "$$ h_{\\theta}(\\mathbf{x}) = \\text{softmax}\\left( W \\cdot \\mathbf{x}  + \\mathbf{b} \\right) $$\n",
    "\n",
    "Where $\\theta$ is a notation for $(W,\\mathbf{b})$.\n",
    "\n",
    "Since this function has to deal with a batched input $\\mathbf{x}$, it's actually represented as a matrix $X$ (also called `x`) of shape `(batch_size, in_dim)`. The weight matrix $W$ (also called `w`) is a matrix of shape `(out_dim, in_dim)`, and the bias term $\\mathbf{b}$ (also called `b`) is a vector of shape `(out_dim,)`.\n",
    "\n",
    "To solve this part, please implement the `softmax_classifier` function in `model.py`. You can test your solution by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z6qHpOU_NqBl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.025s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_model.SoftmaxClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CbWOCqf0vLfe"
   },
   "outputs": [],
   "source": [
    "# playground for debugging softmax_classifier\n",
    "from model import softmax_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLeBVOd_m_f_"
   },
   "source": [
    "## (A.4) Softmax Classifier Backward\n",
    "In this part you will implement the `softmax_classifier_backward`, which computes the gradients of the weights of the Softmax Classifier. Derive the formula for the gradient of $W$ (also called `weight` or `w`), given the input $\\mathbf{x}$ (also called `input` or `x`), the classifier's prediction $\\hat{\\mathbf{y}}$ (also called `pred` or `y_hat`) and the target label $\\ell$ (also called `target` or `y`).\n",
    "\n",
    "To solve this part, please implement the `softmax_classifier_backward` function in `model.py`. You can test your solution by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EpkJmQrGNqdr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_model.SoftmaxClassifierBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "94C1ofpIvjh7"
   },
   "outputs": [],
   "source": [
    "# playground for debugging softmax_classifier_backward\n",
    "from model import softmax_classifier_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiJaEoARnAOn"
   },
   "source": [
    "# (B) Train the Model\n",
    "In this part you will create and train the Softmax Classifier to detect hand-written digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. The dataset consists of images of digits (of size 28√ó28), and their values (0-9) as supervision.\n",
    "\n",
    "The Softmax Classifier should classify over 10 classes, one per digit. The $0$ digit is the class at index `0`. In general, the $d$ digit is the class at index `d`. The output of the classifier is a probablity distribution over the 10 classes. The predicted class is that with highest probability (ties are broken arbitrarily; they are very rare).\n",
    "\n",
    "Your goal is to achieve high accuracy on the test set. Accuracy (you can use the provided `accuracy` function) is defined as the part of examples classified correctly (_i.e._, the predicted class is the correct value of the digit). However, this loss can't be optimized directly - so you'll train the classifier to minimize the Cross-Entropy loss.\n",
    "\n",
    "The classifier is represented as a tuple `(w, b)`. Training the classifier means to update its weights. The training process consists of multiple epochs. In each epoch, the classifier is trained over all the examples in the training test once. Every several epochs, the classifier is tested (_i.e._, evaluted) on the test set. No examples are shared between these sets.\n",
    "\n",
    "You should open the `train.py` file (by clicking on this link: `/content/hw1/train.py`). Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw1` (or `content/hw1`) and double-click on `train.py`. This file contains the following methods:\n",
    "\n",
    "1. `create_model`: You will implement this method to create (and initialize) a model.\n",
    "2. `train_epoch`: You will implement this method to run a single training epoch.\n",
    "3. `test_epoch`: You will implement this method to run a single evaluation (test) epoch.\n",
    "4. `train_loop`: This method is **GIVEN** to you as-is. It uses `train_epoch` and `test_epoch`. You should use it to train your model.\n",
    "\n",
    "You are also recommended to look at the provided utilities file: `/content/hw1/utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUxTJYKM-e76"
   },
   "source": [
    "## (B.0) Load the MNIST Dataset\n",
    "\n",
    "Please run the following cell to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RsT02YM_tGik"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:03<00:00, 2930515.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00<00:00, 182361.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:00<00:00, 1662702.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00<00:00, 2310275.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import load_mnist\n",
    "\n",
    "# Load the training and test sets\n",
    "train_data = load_mnist(mode='train')\n",
    "test_data = load_mnist(mode='test')\n",
    "\n",
    "# Create dataloaders for training and test sets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3padAZndzV_T"
   },
   "source": [
    "## (B.1) Create a Model\n",
    "In this part you will implement a method that creates a new model for MNIST classification (see details above, mainly the input and output sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTKDB2xnzWrW"
   },
   "outputs": [],
   "source": [
    "# playground for debugging create_model\n",
    "from train import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEvLil8Hwoq0"
   },
   "source": [
    "## (B.2) Train a Single Epoch\n",
    "In this part you will implement the `train_epoch` method. This method receives the model `(w, b)`, a learning rate `lr` and a data loader `loader` of the training set, and updates the model weights in order to minimize the cross-entropy loss. It also computes the average loss and accuracy over the training set\\*.\n",
    "\n",
    "You're given a skeleton of this method, mainly the iteration over the data loader. At each iteration (batch) the data loader returns two tensors: `x` and `y`. `x` is a batch of images (has shape `(batch_size, 1, 28, 28)`), and `y` is a batch of (the correct) labels (has shape `(batch_size,)`).\n",
    "\n",
    "In your solution, you should do as follows:\n",
    "\n",
    "1.   Reshape the inputs `x` to match the shape expected by the classifier.\n",
    "2.   Run the model to get a prediction.\n",
    "3.   Compute the cross-entropy loss (**MUST** be stored in a tensor `loss`) and accuracy (**MUST** be stored in a tensor `acc`). You should use the `accuracy` method from `utils.py` (already imported in `train.py`).\n",
    "4.   Run the backward step to compute the gradients of the weights.\n",
    "5.   Update the weights according to their gradients and the learning rate.\n",
    "\n",
    "---\n",
    "\\* This is not enitrely accurate (no pun intended), as the model changes throughout this training phase. This will be different than evaluation of the model over the training set after the training phase. However, since iterating over the training set is expensive, this is the common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3hB0EAnSwbJS"
   },
   "outputs": [],
   "source": [
    "# playground for debugging train_epoch\n",
    "from train import create_model, train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.4653, Accuracy=0.88%\n",
      "Epoch 2: Loss=0.3316, Accuracy=0.91%\n",
      "Epoch 3: Loss=0.3106, Accuracy=0.91%\n",
      "Epoch 4: Loss=0.2997, Accuracy=0.92%\n",
      "Epoch 5: Loss=0.2926, Accuracy=0.92%\n"
     ]
    }
   ],
   "source": [
    "w, b = create_model()\n",
    "w, b = w.to('cpu'), b.to('cpu')\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_metric, acc_metric = train_epoch(w, b, learning_rate, train_loader)\n",
    "    print(f\"Epoch {epoch + 1}: Loss={loss_metric.avg:.4f}, Accuracy={acc_metric.avg:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFrrstK0wpY6"
   },
   "source": [
    "## (B.3) Test After Epoch\n",
    "In this part you will implement the `test_epoch` method. This method recieves the model `(w, b)` and a data loader `loader` of the test set, and computes the average loss and accuracy over it.\n",
    "\n",
    "As before, you're given a skeleton of this function. Note that in `test_epoch` you **MUST NOT** update the model!\n",
    "\n",
    "In your solution, you should do as follows:\n",
    "\n",
    "1.   Reshape the inputs `x` to match the shape expected by the classifier.\n",
    "2.   Run the model to get a prediction.\n",
    "3.   Compute the cross-entropy loss (**MUST** be stored in a tensor `loss`) and accuracy (**MUST** be stored in a tensor `acc`). You should use the `accuracy` method from `utils.py` (already imported in `train.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uL7QMnDKwfQm"
   },
   "outputs": [],
   "source": [
    "# playground for debugging test_epoch\n",
    "from train import create_model, test_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBsWpOJCwp6P"
   },
   "source": [
    "## (B.4) Train A Model\n",
    "In this part you will train your model. You are provided with a `train_loop` method that uses your existing `train_epoch` and `test_epoch`.\n",
    "\n",
    "In this phase, you should:\n",
    "\n",
    "1. Create a model (you may want to check different initialization schemes and see how it changes the convergence speed).\n",
    "2. Set learning rate and number of epochs (you may want to check different parameters and see how they affect the convergence).\n",
    "3. Train your model using `train_loop`. This method reports the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gbW-74LJ2e1m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Epoch: 001 / 010   Loss:  0.4658   Accuracy: 0.877\n",
      " Test   Epoch: 001 / 010   Loss:  0.3331   Accuracy: 0.908\n",
      "Train   Epoch: 002 / 010   Loss:  0.3319   Accuracy: 0.906\n",
      " Test   Epoch: 002 / 010   Loss:  0.3055   Accuracy: 0.914\n",
      "Train   Epoch: 003 / 010   Loss:  0.3108   Accuracy: 0.912\n",
      " Test   Epoch: 003 / 010   Loss:  0.2923   Accuracy: 0.917\n",
      "Train   Epoch: 004 / 010   Loss:  0.2998   Accuracy: 0.915\n",
      " Test   Epoch: 004 / 010   Loss:  0.2863   Accuracy: 0.919\n",
      "Train   Epoch: 005 / 010   Loss:  0.2925   Accuracy: 0.918\n",
      " Test   Epoch: 005 / 010   Loss:  0.2836   Accuracy: 0.919\n",
      "Train   Epoch: 006 / 010   Loss:  0.2873   Accuracy: 0.919\n",
      " Test   Epoch: 006 / 010   Loss:  0.2799   Accuracy: 0.920\n",
      "Train   Epoch: 007 / 010   Loss:  0.2833   Accuracy: 0.920\n",
      " Test   Epoch: 007 / 010   Loss:   0.279   Accuracy: 0.920\n",
      "Train   Epoch: 008 / 010   Loss:  0.2802   Accuracy: 0.921\n",
      " Test   Epoch: 008 / 010   Loss:  0.2757   Accuracy: 0.920\n",
      "Train   Epoch: 009 / 010   Loss:  0.2772   Accuracy: 0.922\n",
      " Test   Epoch: 009 / 010   Loss:  0.2759   Accuracy: 0.921\n",
      "Train   Epoch: 010 / 010   Loss:  0.2751   Accuracy: 0.923\n",
      " Test   Epoch: 010 / 010   Loss:  0.2736   Accuracy: 0.921\n"
     ]
    }
   ],
   "source": [
    "from train import create_model, train_loop\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "# 1. Create a model\n",
    "w, b = create_model()\n",
    "\n",
    "# 2. Set learning rate and number of epochs\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# END SOLUTION\n",
    "\n",
    "# 3. Train your model with `train_loop`\n",
    "train_loop(w=w,\n",
    "           b=b,\n",
    "           lr=lr,\n",
    "           train_loader=train_loader,\n",
    "           test_loader=test_loader,\n",
    "           epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGUEJiToAsn1"
   },
   "source": [
    "# Submit Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kqoyVUMpF5eq"
   },
   "outputs": [],
   "source": [
    "#@title # Create and Download Your Solution\n",
    "\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def create_zip(files, hw, name):\n",
    "  zip_path = f'{hw}-{name}.zip'\n",
    "  with zipfile.ZipFile(zip_path, 'w') as f:\n",
    "    for fname in files:\n",
    "      if not os.path.isfile(fname):\n",
    "        raise FileNotFoundError(f\"Couldn't find file: '{fname}' in the homework directory\")\n",
    "      f.write(fname, fname)\n",
    "  return zip_path\n",
    "\n",
    "# export notebook as html\n",
    "!jupyter nbconvert --to html hw1.ipynb\n",
    "\n",
    "#@markdown Please upload your typed solution (`.pdf` file) to the homework directory, and use the name `hw1-sol.pdf`.\n",
    "\n",
    "student_name = \"John Doe\"  #@param{type:\"string\"}\n",
    "assignment_name = 'hw1'\n",
    "assignment_sol_files = ['hw1-sol.pdf', 'hw1.ipynb', 'hw1.html', 'model.py', 'train.py']\n",
    "zip_name = re.sub('[_ ]+', '_', re.sub(r'[^a-zA-Z_ ]+', '', student_name.lower()))\n",
    "\n",
    "# create zip with your solution\n",
    "zip_path = create_zip(assignment_sol_files, assignment_name, zip_name)\n",
    "\n",
    "# download the zip\n",
    "files.download(zip_path)\n",
    "\n",
    "#@markdown Enter your name in `student_name` and run this cell to create and download a `.zip` file with your solution.\n",
    "\n",
    "#@markdown You should submit your solution via Moodle.\n",
    "\n",
    "#@markdown **Note:** If you run this cell multiple times, you may be prompted by the browser to allow this page to download multiple files."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
